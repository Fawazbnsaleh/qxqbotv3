"""
Fix Mislabeled Training Samples
Corrects ~21 samples identified through comprehensive audit.
Creates a backup before applying changes.
"""
import json
import os
import shutil
from datetime import datetime
from collections import Counter

DATA_PATH = "al_rased/data/labeledSamples/training_data.json"
BACKUP_DIR = "al_rased/data/labeledSamples/backups"

# Each fix: (index, new_label, new_labels, reason)
FIXES = [
    # === Category 1: Labeled as Academic Cheating but should be Normal (discussing, not offering) ===
    (22, "Ø·Ø¨ÙŠØ¹ÙŠ", ["Ø·Ø¨ÙŠØ¹ÙŠ"], "ÙŠØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„Ù†ØµØ§Ø¨ÙŠÙ†ØŒ Ù„ÙŠØ³ Ø¹Ø±Ø¶ ØºØ´"),
    (25, "Ø·Ø¨ÙŠØ¹ÙŠ", ["Ø·Ø¨ÙŠØ¹ÙŠ"], "ÙŠØªØ­Ø¯Ø« Ø¹Ù† Ø¯ÙƒØªÙˆØ±Ù‡ ØªØ­Ù„ ÙˆØ§Ø¬Ø¨Ø§ØªØŒ Ù…Ø¬Ø±Ø¯ Ø¥Ø´Ø§Ø±Ø©"),
    (64, "Ø·Ø¨ÙŠØ¹ÙŠ", ["Ø·Ø¨ÙŠØ¹ÙŠ"], "Ø´ÙƒÙˆÙ‰ Ù…Ù† Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø­Ù„ ÙˆØ§Ø¬Ø¨Ø§Øª ÙÙŠ Ø§Ù„Ù‚Ø±ÙˆØ¨"),
    (87, "Ø·Ø¨ÙŠØ¹ÙŠ", ["Ø·Ø¨ÙŠØ¹ÙŠ"], "Ù…Ø¬Ø±Ø¯ Ø¥Ø´Ø§Ø±Ø© Ø¨Ø³ÙŠØ·Ø© Ù„ÙŠØ³Øª Ø¹Ø±Ø¶"),

    # === Category 2: Labeled as Spam but should be Academic Cheating ===
    (149, "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)", ["ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)"], "Ø¥Ø¹Ù„Ø§Ù† ØªØ¯Ø±ÙŠØ³ Ø®ØµÙˆØµÙŠ ÙˆØ­Ù„ ÙˆØ§Ø¬Ø¨Ø§Øª"),
    (180, "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)", ["ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)"], "Ø®Ø¯Ù…Ø§Øª Ø­Ù„ÙˆÙ„ Ø¯Ø±Ø§Ø³ÙŠØ© Ø¹Ù† Ø¨Ø¹Ø¯"),
    (249, "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)", ["ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)"], "Ø¯ÙƒØªÙˆØ±Ù‡ ØªØ³ÙˆÙŠ Ù…Ø´Ø§Ø±ÙŠØ¹ ÙˆØªØ­Ù„ Ø§Ù…ØªØ­Ø§Ù†Ø§Øª"),
    (444, "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)", ["ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)"], "Ù…Ø¤Ø³Ø³Ø© Ø£Ø¨Ø­Ø§Ø« Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©"),
    (996, "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)", ["ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)"], "Ø¥Ø¹Ù„Ø§Ù† Ù…Ø¯Ø±Ø³Ø© Ø®ØµÙˆØµÙŠØ© ØªÙØ§Ø¶Ù„ ÙˆØªÙƒØ§Ù…Ù„"),

    # === Category 3: Spam but should be Normal ===
    (57, "Ø·Ø¨ÙŠØ¹ÙŠ", ["Ø·Ø¨ÙŠØ¹ÙŠ"], "Ø¨Ø§Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ©ØŒ Ù„ÙŠØ³ Ø³Ø¨Ø§Ù…"),
    (148, "Ø·Ø¨ÙŠØ¹ÙŠ", ["Ø·Ø¨ÙŠØ¹ÙŠ"], "Ø´ÙƒÙˆÙ‰ Ù…Ù† Ø§Ù„Ø¨ÙˆØª ÙˆØ§Ù„Ù…Ø³ØªØ¬Ø¯ÙŠÙ†ØŒ Ù„ÙŠØ³ Ø³Ø¨Ø§Ù…"),

    # === Category 4: Academic Cheating but should be Medical Fraud ===
    (443, "Ø§Ø­ØªÙŠØ§Ù„ Ø·Ø¨ÙŠ (Ø¹Ø±Ø¶)", ["Ø§Ø­ØªÙŠØ§Ù„ Ø·Ø¨ÙŠ (Ø¹Ø±Ø¶)", "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)"],
     "Ø£Ø¹Ø°Ø§Ø± Ø·Ø¨ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"),
    (468, "Ø§Ø­ØªÙŠØ§Ù„ Ø·Ø¨ÙŠ (Ø¹Ø±Ø¶)", ["Ø§Ø­ØªÙŠØ§Ù„ Ø·Ø¨ÙŠ (Ø¹Ø±Ø¶)", "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)"],
     "Ø£Ø¹Ø°Ø§Ø± Ø·Ø¨ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ - Ù…ÙƒØ±Ø± Ù…Ù† 443"),
    (470, "Ø§Ø­ØªÙŠØ§Ù„ Ø·Ø¨ÙŠ (Ø¹Ø±Ø¶)", ["Ø§Ø­ØªÙŠØ§Ù„ Ø·Ø¨ÙŠ (Ø¹Ø±Ø¶)", "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)"],
     "Ø£Ø¹Ø°Ø§Ø± Ø·Ø¨ÙŠØ© Ù…Ù† ØµØ­ØªÙŠ Ù‡ÙŠ Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"),

    # === Category 5: Financial Scams -> Spam or Academic ===
    (536, "Ø³Ø¨Ø§Ù…", ["Ø³Ø¨Ø§Ù…"], "ØªÙ‚Ø³ÙŠØ· Ø¨Ø·Ø§Ù‚Ø§Øª ØªØ¬Ø§Ø±ÙŠ Ù…Ø´Ø¨ÙˆÙ‡ØŒ Ù„ÙŠØ³ Ø§Ø­ØªÙŠØ§Ù„ Ù…Ø§Ù„ÙŠ"),
    (581, "Ø³Ø¨Ø§Ù…", ["Ø³Ø¨Ø§Ù…"], "ØªØ³ÙˆÙŠÙ‚ ØªØ·Ø¨ÙŠÙ‚ Ø¬Ø§Ø²ÙŠØŒ Ù„ÙŠØ³ Ø§Ø­ØªÙŠØ§Ù„"),
    (936, "Ø³Ø¨Ø§Ù…", ["Ø³Ø¨Ø§Ù…"], "Ø¥Ø¹Ù„Ø§Ù† Ø®Ø¯Ù…Ø§Øª ØªØ·ÙˆÙŠØ± Ù…ÙˆØ§Ù‚Ø¹"),
    (992, "Ø³Ø¨Ø§Ù…", ["Ø³Ø¨Ø§Ù…"], "Ø¥Ø¹Ù„Ø§Ù† ØªØ¬Ø§Ø±ÙŠ Ù„Ø´Ø±ÙƒØ© Ø£Ù„Ø¹Ø§Ø¨"),
    (2011, "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)", ["ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)"],
     "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø±Ø³Ø§Ù„Ø© Ù…Ø§Ø¬Ø³ØªÙŠØ± ÙˆØ¯ÙƒØªÙˆØ±Ø§Ù‡"),

    # === Category 6: Offer -> Request ===
    (128, "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø·Ù„Ø¨)", ["ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø·Ù„Ø¨)"], "Ù…Ù† ÙŠØ­Ù„ ÙˆØ§Ø¬Ø¨Ø§Øª = Ø·Ù„Ø¨"),
    (138, "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø·Ù„Ø¨)", ["ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø·Ù„Ø¨)"], "Ù…Ù† ÙŠØ­Ù„ ÙˆØ§Ø¬Ø¨ = Ø·Ù„Ø¨"),
    (141, "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø·Ù„Ø¨)", ["ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø·Ù„Ø¨)"], "ØªØ¹Ø±ÙÙˆÙ† Ø§Ø­Ø¯ ÙŠØ­Ù„ = Ø·Ù„Ø¨"),
]


def main():
    print("=" * 60)
    print("ğŸ”§ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù…ÙˆØ³ÙˆÙ…Ø© Ø¨Ø§Ù„ØºÙ„Ø·")
    print("=" * 60)

    # Load data
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)

    print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª: {len(data)}")

    # Create backup
    os.makedirs(BACKUP_DIR, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = os.path.join(BACKUP_DIR, f"training_data_pre_fix_{ts}.json")
    shutil.copy2(DATA_PATH, backup_path)
    print(f"ğŸ’¾ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {backup_path}")

    # Show before stats
    print("\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø¥ØµÙ„Ø§Ø­:")
    before = Counter(d["label"] for d in data)
    for k, v in before.most_common():
        print(f"  {k}: {v}")

    # Apply fixes
    print(f"\nğŸ”§ ØªØ·Ø¨ÙŠÙ‚ {len(FIXES)} Ø¥ØµÙ„Ø§Ø­...")
    print("-" * 60)

    fix_count = 0
    for idx, new_label, new_labels, reason in FIXES:
        if idx >= len(data):
            print(f"  âš ï¸  [{idx}] Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Ø·Ø§Ù‚ (max={len(data)-1})")
            continue

        sample = data[idx]
        old_label = sample.get("label", "")
        old_labels = sample.get("labels", [])
        text_preview = sample["text"][:50]

        if old_label == new_label and old_labels == new_labels:
            print(f"  â­ï¸  [{idx}] Ø¨Ø§Ù„ÙØ¹Ù„ ØµØ­ÙŠØ­: {old_label}")
            continue

        # Apply fix
        sample["label"] = new_label
        sample["labels"] = new_labels
        sample["note"] = f"Mislabel Fix: {old_label} -> {new_label} ({reason})"
        sample["reviewed_at"] = datetime.now().isoformat()
        sample["reviewed_by"] = "mislabel_audit_feb2026"

        fix_count += 1
        print(f"  âœ… [{idx}] {old_label} -> {new_label}")
        print(f"       Ø§Ù„Ù†Øµ: {text_preview}...")
        print(f"       Ø§Ù„Ø³Ø¨Ø¨: {reason}")

    # Save
    with open(DATA_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    # Show after stats
    print(f"\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¨Ø¹Ø¯ Ø§Ù„Ø¥ØµÙ„Ø§Ø­:")
    after = Counter(d["label"] for d in data)
    for k, v in after.most_common():
        diff = v - before.get(k, 0)
        marker = f" ({'+' if diff > 0 else ''}{diff})" if diff != 0 else ""
        print(f"  {k}: {v}{marker}")

    print(f"\nâœ… ØªÙ… Ø¥ØµÙ„Ø§Ø­ {fix_count} Ø¹ÙŠÙ†Ø© Ø¨Ù†Ø¬Ø§Ø­.")
    print(f"ğŸ’¾ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ: {backup_path}")


if __name__ == "__main__":
    main()

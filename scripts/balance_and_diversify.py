#!/usr/bin/env python3
"""
BALANCE & DIVERSIFY DATASET
1. Intensive mining for weak categories
2. Reduce Normal class to improve balance
3. Remove over-duplicated samples
"""
import json
import os
import glob
import random
from collections import Counter

print('âš–ï¸ BALANCING & DIVERSIFYING DATASET')
print('=' * 70)

# ========== 1. INTENSIVE MINING ==========
print('\nğŸ“¥ 1. INTENSIVE MINING FOR WEAK CATEGORIES')
print('-' * 40)

# Extended patterns with more variety
mining_patterns = {
    'Hacking': [
        # Arabic hacking services
        'Ù‡ÙƒØ±', 'ØªÙ‡ÙƒÙŠØ±', 'Ø§Ø®ØªØ±Ø§Ù‚', 'Ø³Ø±Ù‚Ø© Ø­Ø³Ø§Ø¨', 'ÙÙƒ Ø­Ù…Ø§ÙŠØ©',
        'Ø­Ø¸Ø± Ø­Ø³Ø§Ø¨', 'ÙØªØ­ Ø­Ø³Ø§Ø¨ Ù…Ø­Ø¸ÙˆØ±', 'Ø³Ø­Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª', 'ØªØ¬Ø³Ø³',
        'Ø¨Ø§Ø³ÙˆØ±Ø¯', 'ÙƒÙ„Ù…Ø© Ø³Ø±', 'Ø±Ù…Ø² Ø§Ù„ØªØ­Ù‚Ù‚', 'ÙƒÙˆØ¯ Ø§Ù„ØªÙØ¹ÙŠÙ„',
        # English hacking terms
        'hack', 'hacker', 'crack', 'bypass', 'bruteforce',
        # Platform-specific
        'ÙˆØ§ØªØ³ Ù‡ÙƒØ±', 'Ø§Ù†Ø³ØªØ§ Ù‡ÙƒØ±', 'ØªÙ„ÙŠØ¬Ø±Ø§Ù… Ù‡ÙƒØ±', 'Ø³Ù†Ø§Ø¨ Ù‡ÙƒØ±',
    ],
    'Spam': [
        # Followers/Likes services
        'Ø²ÙŠØ§Ø¯Ø© Ù…ØªØ§Ø¨Ø¹ÙŠÙ†', 'Ø±Ø´Ù‚ Ù…ØªØ§Ø¨Ø¹ÙŠÙ†', 'Ø´Ø±Ø§Ø¡ Ù…ØªØ§Ø¨Ø¹ÙŠÙ†', 'Ù…ØªØ§Ø¨Ø¹ÙŠÙ† ÙˆÙ‡Ù…ÙŠÙŠÙ†',
        'Ø²ÙŠØ§Ø¯Ø© Ù„Ø§ÙŠÙƒØ§Øª', 'Ø²ÙŠØ§Ø¯Ø© Ù…Ø´Ø§Ù‡Ø¯Ø§Øª', 'ØªØ±ÙˆÙŠØ¬ Ø­Ø³Ø§Ø¨',
        # Subscription services
        'Ø§Ø´ØªØ±Ø§ÙƒØ§Øª', 'Ù†ØªÙÙ„ÙƒØ³', 'Ø´Ø§Ù‡Ø¯', 'spotify', 'iptv',
        # Generic spam
        'Ø´Ø­Ù† Ø¬ÙˆØ§Ù‡Ø±', 'Ø´Ø­Ù† Ø´Ø¯Ø§Øª', 'Ø´Ø­Ù† Ø§Ù„Ù…Ø§Ø³', 'Ø´Ø­Ù† uc',
        # Ads
        'Ù„Ù„Ø§Ø¹Ù„Ø§Ù†', 'Ù„Ù„Ø¥Ø¹Ù„Ø§Ù†', 'Ø§Ø¹Ù„Ø§Ù†Ø§Øª', 'Ø¯Ø¹Ø§ÙŠØ©',
    ],
    'Financial Scams': [
        # Investment scams
        'Ø§Ø³ØªØ«Ù…Ø§Ø± Ù…Ø¶Ù…ÙˆÙ†', 'Ø§Ø±Ø¨Ø§Ø­ ÙŠÙˆÙ…ÙŠØ©', 'Ø§Ø±Ø¨Ø§Ø­ Ù…Ø¶Ù…ÙˆÙ†Ø©', 'Ø¹ÙˆØ§Ø¦Ø¯',
        'ØªØ¯Ø§ÙˆÙ„', 'ÙÙˆØ±ÙƒØ³', 'forex', 'Ø±Ø¨Ø­ Ø³Ø±ÙŠØ¹', 'Ø«Ø±ÙˆØ©',
        # Job scams
        'ÙˆØ¸ÙŠÙØ© Ù…Ù† Ø§Ù„Ù…Ù†Ø²Ù„', 'Ø±Ø§ØªØ¨ Ø¨Ø¯ÙˆÙ† Ø¹Ù…Ù„', 'Ø¯Ø®Ù„ Ø§Ø¶Ø§ÙÙŠ',
        # Crypto scams
        'Ø¹Ù…Ù„Ø§Øª Ø±Ù‚Ù…ÙŠØ©', 'Ø¨ØªÙƒÙˆÙŠÙ†', 'Ø§ÙŠØ«Ø±ÙŠÙˆÙ…', 'Ø¹Ù…Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©',
    ],
}

# Load current data
file_path = 'al_rased/data/labeledSamples/training_data.json'
with open(file_path, 'r') as f:
    data = json.load(f)

existing_texts = {d['text'] for d in data}
mined_counts = {cat: 0 for cat in mining_patterns}

# Search in group_messages
group_messages_path = 'al_rased/data/group_messages'
if os.path.exists(group_messages_path):
    json_files = glob.glob(f'{group_messages_path}/*.json')
    
    for json_file in json_files:
        try:
            with open(json_file, 'r') as f:
                messages = json.load(f)
            
            for msg in messages:
                if not isinstance(msg, dict):
                    continue
                
                txt = msg.get('text', '')
                txt_lower = txt.lower()
                
                if len(txt) < 30 or txt in existing_texts:
                    continue
                
                # Check for patterns - must be a SERVICE not a question
                is_question = any(q in txt_lower for q in ['Ù…ÙŠÙ† ÙŠØ¹Ø±Ù', 'Ø§Ø¨ÙŠ', 'Ø§Ø¨ØºÙ‰', 'ÙƒÙŠÙ'])
                is_service = any(s in txt_lower for s in ['Ù…ØªÙˆÙØ±', 'Ù„Ù„ØªÙˆØ§ØµÙ„', 'ÙŠÙˆØ¬Ø¯ Ù„Ø¯ÙŠÙ†Ø§', 'Ø®Ø§Øµ', 'dm', 'Ù„Ù„Ø·Ù„Ø¨'])
                
                if is_question and not is_service:
                    continue
                
                for category, patterns in mining_patterns.items():
                    if any(p in txt_lower for p in patterns):
                        data.append({
                            'text': txt,
                            'label': category,
                            'source': 'intensive_mining'
                        })
                        existing_texts.add(txt)
                        mined_counts[category] += 1
                        break
        except:
            continue

for cat, count in mined_counts.items():
    if count > 0:
        print(f'   {cat}: +{count} new samples')

# ========== 2. REDUCE NORMAL CLASS ==========
print('\nğŸ“‰ 2. REDUCING NORMAL CLASS (Target: 1000)')
print('-' * 40)

normal_samples = [d for d in data if d['label'] == 'Normal']
other_samples = [d for d in data if d['label'] != 'Normal']

# Keep diverse Normal samples (prioritize longer, more diverse ones)
random.shuffle(normal_samples)  # Shuffle first to avoid bias
normal_samples.sort(key=lambda x: len(x['text']), reverse=True)  # Keep longer ones

# Keep top 1000
target_normal = 1000
if len(normal_samples) > target_normal:
    removed = len(normal_samples) - target_normal
    normal_samples = normal_samples[:target_normal]
    print(f'   Reduced Normal: {len(normal_samples) + removed} -> {len(normal_samples)} (-{removed})')
else:
    print(f'   Normal already at {len(normal_samples)} (no reduction needed)')

data = other_samples + normal_samples

# ========== 3. REMOVE OVER-DUPLICATED ==========
print('\nğŸ” 3. REMOVING EXCESSIVE DUPLICATES')
print('-' * 40)

# For upsampled categories, limit duplicates to max 2x original
for cat in ['Hacking', 'Spam', 'Unethical']:
    cat_samples = [d for d in data if d['label'] == cat]
    original = [d for d in cat_samples if not d.get('is_augmented') and not d.get('upsampled_by')]
    augmented = [d for d in cat_samples if d.get('is_augmented') or d.get('upsampled_by')]
    
    # Max augmented = 2x original
    max_augmented = len(original) * 2
    if len(augmented) > max_augmented:
        removed = len(augmented) - max_augmented
        augmented = random.sample(augmented, max_augmented)
        print(f'   {cat}: Reduced augmented from {len(augmented) + removed} to {len(augmented)} (-{removed})')
        
        # Rebuild data
        other = [d for d in data if d['label'] != cat]
        data = other + original + augmented

# ========== SAVE ==========
with open(file_path, 'w') as f:
    json.dump(data, f, indent=2, ensure_ascii=False)

# ========== FINAL STATS ==========
print('\n' + '=' * 70)
print('ğŸ“Š FINAL DISTRIBUTION:')
labels = Counter(d['label'] for d in data)
total = len(data)
for lbl, cnt in labels.most_common():
    pct = cnt / total * 100
    bar = 'â–ˆ' * int(pct / 2)
    print(f'   {lbl:20} {cnt:5} ({pct:5.1f}%) {bar}')

# Balance score
min_count = min(labels.values())
max_count = max(labels.values())
balance = min_count / max_count * 100
print(f'\nâš–ï¸ Balance Ratio: {balance:.1f}%')

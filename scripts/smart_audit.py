
import json
import re
import os
import sys
from datetime import datetime
from collections import Counter

# Add parent path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from al_rased.features.detection.engine import DetectionEngine
from al_rased.features.detection.handlers import get_thresholds

# Expert Rules (Regex Patterns)
EXPERT_RULES = {
    'Academic Cheating': [
        r'Ø­Ù„\s*ÙˆØ§Ø¬Ø¨', r'Ø­Ù„\s*Ø§Ø®ØªØ¨Ø§Ø±', r'Ù…Ø´Ø§Ø±ÙŠØ¹\s*ØªØ®Ø±Ø¬', r'Ø±Ø³Ø§Ø¦?Ù„\s*Ù…Ø§Ø¬Ø³ØªÙŠØ±', 
        r'Ø§Ø¹Ø¯Ø§Ø¯\s*Ø¨Ø­ÙˆØ«', r'Ø®Ø¯Ù…Ø§Øª\s*Ø·Ù„Ø§Ø¨ÙŠØ©', r'Ø§Ø³Ø§ÙŠÙ…Ù†Øª', r'ÙƒÙˆÙŠØ²Ø§Øª', r'ØªØ³Ù…ÙŠØ¹',
        r'Ø­Ù„ÙˆÙ„\s*ÙˆØ§Ø¬Ø¨Ø§Øª', r'Ù…Ø³Ø§Ø¹Ø¯Ø©\s*ÙÙŠ\s*Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±', r'Ù‚Ø±ÙˆØ¨\s*Ø­Ù„', r'Ø£Ø¨Ø­Ø§Ø«\s*Ø¬Ø§Ù…Ø¹ÙŠØ©'
    ],
    'Medical Fraud': [
        r'Ø³ÙƒÙ„ÙŠÙ', r'Ø§Ø¬Ø§Ø²Ø©\s*Ù…Ø±Ø¶ÙŠØ©', r'ØªÙ‚Ø±ÙŠØ±\s*Ø·Ø¨ÙŠ', r'Ø¹Ø°Ø±\s*Ø·Ø¨ÙŠ', r'Ù…Ø´Ù‡Ø¯\s*Ù…Ø±Ø§ÙÙ‚Ø©',
        r'Ù…Ø³ØªØ´ÙÙ‰\s*Ø­ÙƒÙˆÙ…ÙŠ', r'Ù…Ù†ØµØ©\s*ØµØ­ØªÙŠ', r'ØªØ·Ø¨ÙŠÙ‚\s*ØµØ­ØªÙŠ', r'Ù…Ø±Ø¶ÙŠÙ‡\s*Ù…Ø¹ØªÙ…Ø¯Ù‡'
    ],
    'Financial Scams': [
        r'Ø§Ø³ØªØ«Ù…[Ø±Ø§Ø±]', r'Ø§Ø±Ø¨Ø§Ø­\s*Ù…Ø¶Ù…ÙˆÙ†Ø©', r'ØªØ¯Ø§ÙˆÙ„', r'ÙÙˆØ±ÙƒØ³', r'Ø¹Ù…Ù„Ø§Øª\s*Ø±Ù‚Ù…ÙŠØ©',
        r'Ø§Ø¯Ø§Ø±Ø©\s*Ù…Ø­Ø§ÙØ¸', r'Ø±Ø¨Ø­\s*ÙŠÙˆÙ…ÙŠ', r'Ø¯Ø®Ù„\s*Ø§Ø¶Ø§ÙÙŠ', r'ØªÙˆØµÙŠØ§Øª\s*Ø°Ù‡Ø¨'
    ],
    'Hacking': [
        r'ØªÙ‡ÙƒÙŠØ±', r'Ø§Ø®ØªØ±Ø§Ù‚', r'ØªØ¬Ø³Ø³', r'Ø³Ø­Ø¨\s*ØµÙˆØ±', r'Ø§Ø³ØªØ±Ø¯Ø§Ø¯\s*Ø­Ø³Ø§Ø¨',
        r'Ø²ÙŠØ§Ø¯Ø©\s*Ù…ØªØ§Ø¨Ø¹ÙŠÙ†', r'ØªÙˆØ«ÙŠÙ‚\s*Ø­Ø³Ø§Ø¨'
    ],
    'Unethical': [
        r'Ø³ÙƒØ³', r'Ù†ÙŠ[ÙƒÚª]', r'Ù…Ù…Ø­ÙˆÙ†', r'Ø¯ÙŠÙˆØ«', r'Ù‚Ø­Ø¨Ø©', r'Ø³Ù‡Ø±Ø§Øª', r'Ù…Ø³Ø§Ø¬', r'Ù…Ø¯Ù„Ø¹Ø©',
        r'Ø­Ø´ÙŠØ´', r'Ù…Ø®Ø¯Ø±Ø§Øª', r'ÙƒØ¨ØªØ§Ø¬ÙˆÙ†', r'Ø´Ø¨Ùˆ'
    ],
    'Spam': [
        r'Ø³ÙŠØ±ÙØ±\s*Ù…Ø§ÙŠÙ†ÙƒØ±Ø§ÙØª', r'ØªØ¨Ø§Ø¯Ù„\s*Ù†Ø´Ø±', r'Ø§Ø´ØªØ±Ùƒ\s*ÙÙŠ\s*Ù‚Ù†Ø§ØªÙ†Ø§', r'Ø²ÙŠØ§Ø¯Ø©\s*Ù…ØªØ§Ø¨Ø¹ÙŠÙ†',
        r'Ø§Ø±Ù‚Ø§Ù…\s*ÙˆÙ‡Ù…ÙŠØ©', r'ØªÙØ¹ÙŠÙ„\s*ØªÙ„ÙŠØ¬Ø±Ø§Ù…', r'Ø±Ø´Ù‚'
    ]
}

def check_expert_rules(text):
    text = text.lower()
    matches = []
    for label, patterns in EXPERT_RULES.items():
        for pattern in patterns:
            if re.search(pattern, text):
                matches.append({'label': label, 'pattern': pattern})
                break 
    return matches

def main():
    print("ğŸ” Starting Smart Audit...")
    
    # Load Data
    data_path = 'al_rased/data/labeledSamples/training_data.json'
    with open(data_path, 'r') as f:
        data = json.load(f)
    print(f"ğŸ“Š Loaded {len(data)} samples")

    # Load Model
    print("ğŸ¤– Loading Model...")
    DetectionEngine.load_model()
    thresholds = get_thresholds()

    current_time = datetime.now().strftime("%Y-%m-%d %H:%M")
    report = [f"# ğŸ§ Smart Audit Report - {current_time}\n"]

    mismatches = []
    weak_samples = []
    pattern_matches = []
    
    # Analyze
    for i, sample in enumerate(data):
        text = sample['text']
        current_labels = sample.get('labels', [sample.get('label', 'Normal')])
        if isinstance(current_labels, str): current_labels = [current_labels]
        
        # 1. Model Prediction Check
        pred = DetectionEngine.predict(text)
        pred_label = str(pred['label'])
        confidence = pred['confidence']
        threshold = thresholds.get(pred_label, 0.5)
        
        # 2. Expert Rules Check
        rules = check_expert_rules(text)
        rule_labels = [m['label'] for m in rules]
        
        # A. Label Validation (Critical Errors)
        # Case: Label says Normal/Other, but Expert Rule says VIOLATION
        for match in rules:
            if match['label'] not in current_labels:
                mismatches.append({
                    'id': i,
                    'text': text[:100],
                    'current': current_labels,
                    'suggested': match['label'],
                    'reason': f"Expert Rule: {match['pattern']}",
                    'confidence': confidence
                })

        # Case: Label is Violation, but text seems Normal (harder, check confidence)
        if 'Normal' not in current_labels and not rules and confidence < 0.6:
             # If no expert keyword found and model is low confidence, might be false positive
             weak_samples.append({
                'id': i,
                'text': text[:100],
                'current': current_labels,
                'reason': "Low confidence violation without expert keyword match",
                'confidence': confidence
             })

        # B. Weak Samples (Low Confidence / Notes)
        note = sample.get('note', '').lower()
        if 'weak' in note or 'false' in note or (confidence > threshold - 0.15 and confidence < threshold + 0.1):
             # Gray zone / flagged samples
             pass # Already covered by gray zone logic, but let's log if not duplicate

        # C. Pattern Detection (Suspicious Normal)
        if 'Normal' in current_labels:
            # Check for patterns that fool model
            # E.g., repeated content
            if len(text) > 100 and len(set(text)) < 20: # Spammy repetition
                pattern_matches.append({
                    'id': i,
                    'text': text[:100],
                    'type': "Repetitive Spam",
                    'current': "Normal"
                })
            # Emojis overload
            emoji_count = len(re.findall(r'[^\w\s,\.]', text))
            if emoji_count > 10 and len(text) < 200:
                pattern_matches.append({
                    'id': i,
                    'text': text[:100],
                    'type': "Emoji Spam",
                    'current': "Normal"
                })

    # Generate Report
    
    # 1. Mismatches (Critical)
    report.append("## ğŸš¨ Critical Mismatches (Label vs Expert Rules)")
    report.append(f"Found **{len(mismatches)}** potential errors.\n")
    if mismatches:
        report.append("| Text Snippet | Current Label | Suggested Label | Reason |")
        report.append("|--------------|---------------|-----------------|--------|")
        for m in mismatches[:20]: # Limit output
            report.append(f"| {m['text'].replace('|', '')}... | {m['current']} | **{m['suggested']}** | {m['reason']} |")
    
    # 2. Weak Samples
    report.append("\n## âš ï¸ Weak / Ambiguous Samples")
    report.append(f"Found **{len(weak_samples)}** weak violations.\n")
    if weak_samples:
        report.append("| Text Snippet | Current Label | Confidence | Reason |")
        report.append("|--------------|---------------|------------|--------|")
        for m in weak_samples[:10]:
            report.append(f"| {m['text'].replace('|', '')}... | {m['current']} | {m['confidence']:.2f} | {m['reason']} |")
            
    # 3. Suspicious Patterns
    report.append("\n## ğŸ•µï¸ Suspicious Patterns in 'Normal'")
    report.append(f"Found **{len(pattern_matches)}** suspicious samples.\n")
    if pattern_matches:
        for m in pattern_matches[:5]:
            report.append(f"- **{m['type']}**: `{m['text']}...`")

    # 4. Note Field Improvement
    report.append("\n## ğŸ’¡ Note Field Strategy")
    report.append("""
Current 'note' usage is sparse. Recommendation for Active Learning:
- Auto-populate 'note' when fixing samples (e.g., "Fixed: Keyword 'X' detected").
- Use structured tags in note: `#FalsePositive`, `#WeakSignal`, `#KeywordMismatch`.
- During review, if User changes label, prompt for reason and save to 'note'.
    """)
    
    # 5. Quality Score
    total_issues = len(mismatches) + len(weak_samples) + len(pattern_matches)
    quality_score = max(0, 100 - (total_issues / len(data) * 100 * 5)) # Penalty factor
    report.append(f"\n## ğŸ† Dataset Quality Score: **{quality_score:.1f}/100**")
    report.append(f"- Total Samples: {len(data)}")
    report.append(f"- Total Issues Found: {total_issues}")

    # Write Report
    with open('smart_audit_report.md', 'w') as f:
        f.write('\n'.join(report))
    
    print(f"âœ… Audit Complete. Report saved to smart_audit_report.md")
    print(f"Found {len(mismatches)} mismatches and {len(weak_samples)} weak samples.")

if __name__ == "__main__":
    main()

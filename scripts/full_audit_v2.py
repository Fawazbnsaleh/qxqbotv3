"""
Full Comprehensive Audit of ALL Training Samples (v2)
Reads every sample and checks for mislabels using keyword rules,
contextual analysis, and cross-category validation.
"""
import json
import re
from collections import Counter, defaultdict

DATA_PATH = "al_rased/data/labeledSamples/training_data.json"

# ===== KEYWORD PATTERNS FOR EACH CATEGORY =====

# Academic Cheating (Offer) - someone offering to solve homework
ACADEMIC_OFFER_PATTERNS = [
    re.compile(r"(Ø§Ø­Ù„|Ø¨Ø­Ù„|Ù†Ø­Ù„|Ø§Ø³ÙˆÙŠ|Ø¨Ø³ÙˆÙŠ|Ù†Ø³ÙˆÙŠ|Ø§Ø¹Ù…Ù„|Ø¨Ø¹Ù…Ù„|Ù†Ø¹Ù…Ù„).{0,20}(ÙˆØ§Ø¬Ø¨|Ø¨Ø­Ø«|Ø¨Ø­ÙˆØ«|Ù…Ø´Ø±ÙˆØ¹|ØªÙ‚Ø±ÙŠØ±|ØªÙƒÙ„ÙŠÙ|ØªÙƒØ§Ù„ÙŠÙ|Ø§Ø®ØªØ¨Ø§Ø±|ÙƒÙˆÙŠØ²|Ø§Ù…ØªØ­Ø§Ù†|Ø¨Ø±Ø²Ù†ØªÙŠØ´Ù†|Ø¨ÙˆØ±Ø¨ÙˆÙŠÙ†Øª)", re.I),
    re.compile(r"(ÙˆØ§Ø¬Ø¨|Ø¨Ø­Ø«|Ø¨Ø­ÙˆØ«|Ù…Ø´Ø±ÙˆØ¹|ØªÙ‚Ø±ÙŠØ±|ØªÙƒØ§Ù„ÙŠÙ|Ø§Ø®ØªØ¨Ø§Ø±|ÙƒÙˆÙŠØ²|Ø§Ù…ØªØ­Ø§Ù†).{0,20}(ÙÙ„ Ù…Ø§Ø±Ùƒ|ÙÙ„Ù…Ø§Ø±Ùƒ|Ù…Ø¶Ù…ÙˆÙ†|Ø§Ù„Ø¯Ø±Ø¬Ù‡|Ø§Ù„Ø¯Ø±Ø¬Ø©)", re.I),
    re.compile(r"(Ø®Ø¯Ù…Ø§Øª|Ù…Ø³Ø§Ø¹Ø¯).{0,15}(Ø·Ù„Ø§Ø¨|Ø·Ø§Ù„Ø¨|Ø¯Ø±Ø§Ø³|Ø¬Ø§Ù…Ø¹|Ø§ÙƒØ§Ø¯ÙŠÙ…)", re.I),
    re.compile(r"(Ø­Ù„ÙˆÙ„|Ø­Ù„).{0,15}(Ø¯Ø±Ø§Ø³|Ø¬Ø§Ù…Ø¹|Ø§ÙƒØ§Ø¯ÙŠÙ…)", re.I),
    re.compile(r"(Ù…Ø¹Ù„Ù…|Ù…Ø¯Ø±Ø³|Ø¯ÙƒØªÙˆØ±).{0,20}(ÙŠØ­Ù„|ØªØ­Ù„|Ø­Ù„|ÙˆØ§Ø¬Ø¨|Ø§Ø®ØªØ¨Ø§Ø±)", re.I),
    re.compile(r"(Ø§Ù„Ù„ÙŠ|Ø§Ù„ÙŠ) ÙŠØ¨ÙŠ.{0,20}(Ø­Ù„|ÙˆØ§Ø¬Ø¨|Ø¨Ø­Ø«|Ù…Ø´Ø±ÙˆØ¹|ØªÙƒØ§Ù„ÙŠÙ)", re.I),
    re.compile(r"(Ø¨Ø­ÙˆØ«|Ù…Ø´Ø§Ø±ÙŠØ¹|ÙˆØ§Ø¬Ø¨Ø§Øª).{0,15}(ØªØ®Ø±Ø¬|Ø¬Ø§Ù…Ø¹|Ø¯Ø±Ø§Ø³)", re.I),
    re.compile(r"Ø®Ø¯Ù…Ø§Øª.{0,10}(Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠ|Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ).{0,30}(Ø­Ù„|ÙˆØ§Ø¬Ø¨|Ø¨Ø­Ø«|Ù…Ø´Ø±ÙˆØ¹|ØªÙƒØ§Ù„ÙŠÙ|ØªÙ‚Ø±ÙŠØ±)", re.I),
    re.compile(r"(Ù†Ù‚Ø¯Ù…|Ù†Ù‚Ø¯Ù‘Ù…).{0,30}(Ø­Ù„|ÙˆØ§Ø¬Ø¨|Ø¨Ø­Ø«|Ù…Ø´Ø±ÙˆØ¹|ØªÙƒØ§Ù„ÙŠÙ|ØªÙ‚Ø±ÙŠØ±|Ø¯Ø±Ø§Ø³)", re.I),
]

# Academic Cheating (Request) - someone asking for help solving
ACADEMIC_REQUEST_PATTERNS = [
    re.compile(r"(Ù…Ù†|Ù…Ù†Ùˆ|ÙÙŠÙ‡ Ø§Ø­Ø¯|Ø§Ø­Ø¯|Ù‡Ù„ ÙÙŠÙ‡|ØªØ¹Ø±ÙÙˆÙ†|ÙŠØ¹Ø±Ù).{0,15}(ÙŠØ­Ù„|ØªØ­Ù„|ÙŠØ³ÙˆÙŠ|ØªØ³ÙˆÙŠ|ÙŠØ³Ø§Ø¹Ø¯|ØªØ³Ø§Ø¹Ø¯).{0,15}(ÙˆØ§Ø¬Ø¨|Ø¨Ø­Ø«|Ù…Ø´Ø±ÙˆØ¹|ØªÙ‚Ø±ÙŠØ±|ØªÙƒÙ„ÙŠÙ|Ø§Ø®ØªØ¨Ø§Ø±|ÙƒÙˆÙŠØ²)", re.I),
    re.compile(r"(Ø§Ø¨ÙŠ|Ø§Ø¨ØºÙ‰|Ù…Ø­ØªØ§Ø¬|ÙŠØ¨ÙŠ).{0,15}(Ø§Ø­Ø¯|Ø­Ø¯|Ø´Ø®Øµ).{0,10}(ÙŠØ­Ù„|ÙŠØ³ÙˆÙŠ|ÙŠØ³Ø§Ø¹Ø¯)", re.I),
    re.compile(r"(Ù…Ù† ÙŠØ­Ù„|Ù…Ù† ØªØ­Ù„|Ù…Ù† ÙŠÙ‚Ø¯Ø± ÙŠØ­Ù„)", re.I),
    re.compile(r"(Ø§Ù„ÙŠ ÙŠØ­Ù„|Ø§Ù„Ù„ÙŠ ÙŠØ­Ù„).{0,10}(ÙŠØ±Ø³Ù„|ÙŠØªÙˆØ§ØµÙ„|ÙŠÙƒÙ„Ù…)", re.I),
]

# Medical Fraud - selling fake sick leaves / medical excuses
MEDICAL_FRAUD_PATTERNS = [
    re.compile(r"(Ø³ÙƒÙ„ÙŠÙ|Ø³ÚªÙ„ÙŠÙ|Ø³ÙƒÙ„Ù|Ø´ÙƒÙ„ÙŠÙ|Ø³ÙƒØ§Ù„ÙŠÙ|Ø³ÙƒØ§Ù„Ù|Ø³Ú¯Ù„ÙŠÙ)", re.I),
    re.compile(r"(Ø¹Ø°Ø±|Ø§Ø¹Ø°Ø§Ø±|Ø£Ø¹Ø°Ø§Ø±).{0,10}(Ø·Ø¨|Ù…Ø±Ø¶)", re.I),
    re.compile(r"(Ø§Ø¬Ø§Ø²Ø©|Ø§Ø¬Ø§Ø²Ù‡|Ø¥Ø¬Ø§Ø²Ø©|Ø¥Ø¬Ø§Ø²Ù‡).{0,10}(Ù…Ø±Ø¶|Ø·Ø¨)", re.I),
    re.compile(r"(ØµØ­ØªÙŠ|ØµØ­ØªÙ‰|Ù…Ù†ØµØ© ØµØ­ØªÙŠ)", re.I),
    re.compile(r"(Ù†Ø·Ù„Ø¹|Ø§Ø·Ù„Ø¹|ØªØ³ÙˆÙŠ|Ù†Ø³ÙˆÙŠ|Ø§Ø³ÙˆÙŠ).{0,15}(Ø§Ø¹Ø°Ø§Ø±|Ø³ÙƒÙ„ÙŠÙ|Ø³ÙƒÙ„Ù)", re.I),
    re.compile(r"(ØºÙŠØ§Ø¨|ØªØºÙŠØ¨).{0,20}(Ø§Ø¹Ø°Ø§Ø±|Ø¹Ø°Ø±|Ø³ÙƒÙ„ÙŠÙ|Ø·Ø¨|Ù…Ø±Ø¶)", re.I),
]

# Financial Scams - investment fraud, get-rich-quick schemes
FINANCIAL_SCAM_PATTERNS = [
    re.compile(r"(Ø§Ø³ØªØ«Ù…Ø±|Ø§Ø³ØªØ«Ù…Ø§Ø±|invest).{0,15}(Ù…Ø¹|Ù…Ø¹ÙŠ|Ù…Ø¹Ù†Ø§|Ø§Ù„Ø§Ù†|Ø§Ù„Ø¢Ù†|ÙØ±Øµ)", re.I),
    re.compile(r"(Ø§Ø±Ø¨Ø§Ø­|Ø£Ø±Ø¨Ø§Ø­|Ø±Ø¨Ø­|Ø¹ÙˆØ§Ø¦Ø¯).{0,15}(Ù…Ø¶Ù…ÙˆÙ†|ÙŠÙˆÙ…ÙŠ|Ø´Ù‡Ø±ÙŠ|Ø¨Ø¯ÙˆÙ†)", re.I),
    re.compile(r"(ÙÙˆØ±ÙƒØ³|forex|ØªØ¯Ø§ÙˆÙ„ Ø¹Ù…Ù„Ø§Øª|crypto|ÙƒØ±ÙŠØ¨ØªÙˆ)", re.I),
    re.compile(r"(Ø¯Ø®Ù„|Ø±Ø¨Ø­).{0,10}(Ø§Ø¶Ø§ÙÙŠ|Ø¥Ø¶Ø§ÙÙŠ|Ù…Ù† Ø§Ù„Ø¨ÙŠØª|Ø¨Ø¯ÙˆÙ† Ù…Ø¬Ù‡ÙˆØ¯)", re.I),
    re.compile(r"(ÙØ±ØµØ©|ÙØ±Øµ).{0,10}(Ø°Ù‡Ø¨ÙŠ|Ø§Ø³ØªØ«Ù…Ø§Ø±|Ø±Ø¨Ø­)", re.I),
    re.compile(r"(ETH|BTC|USDT|bitcoin|ethereum).{0,20}(giveaway|free|airdrop)", re.I),
]

# Spam - generic advertisements, promotions, group joins
SPAM_PATTERNS = [
    re.compile(r"(Ø³ÙŠØ±ÙØ±|Ø±ÙŠÙ„Ù…).{0,10}(Ù…Ø§ÙŠÙ†ÙƒØ±Ø§ÙØª|Ù…Ø§ÙŠÙ† ÙƒØ±Ø§ÙØª|ÙƒØ±Ø§ÙØª|Ø¯ÙŠØ³ÙƒÙˆØ±Ø¯)", re.I),
    re.compile(r"(Ø´Ø­Ù†|Ø±Ø´Ù‚).{0,10}(Ù…ØªØ§Ø¨Ø¹ÙŠÙ†|Ù„Ø§ÙŠÙƒØ§Øª|Ø´Ø¯Ø§Øª|Ø¬ÙˆØ§Ù‡Ø±)", re.I),
    re.compile(r"(Ø§Ø´ØªØ±Ø§Ùƒ|Ù‚Ø³Ø§Ø¦Ù…|ÙƒÙˆØ¨ÙˆÙ†|ÙƒÙˆØ¨ÙˆÙ†Ø§Øª|Ù‚Ø³ÙŠÙ…Ø©)", re.I),
    re.compile(r"(Ù…ØªØ¬Ø±|Ø³ØªÙˆØ±|shop).{0,15}(Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠ|Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ|Ø§ÙˆÙ† Ù„Ø§ÙŠÙ†|online)", re.I),
    re.compile(r"(ØªÙ‚Ø³ÙŠØ·|Ù‚Ø³Ø·).{0,15}(Ø¨Ø·Ø§Ù‚|Ø¬ÙˆØ§Ù„|Ø§ÙŠÙÙˆÙ†|Ø¢ÙŠÙÙˆÙ†|Ø³Ø§Ù…Ø³ÙˆÙ†Ø¬)", re.I),
    re.compile(r"(Ø³Ø¬Ù„|Ø§Ù†Ø¶Ù…|Ø§Ø¯Ø®Ù„).{0,15}(Ø±Ø§Ø¨Ø·|Ù„ÙŠÙ†Ùƒ|Ù‚Ø±ÙˆØ¨|Ù‚Ù†Ø§Ø©|link)", re.I),
]

# Hacking - hacking services
HACKING_PATTERNS = [
    re.compile(r"(ØªÙ‡ÙƒÙŠØ±|Ø§Ø®ØªØ±Ø§Ù‚|Ù‡ÙƒØ±|Ù‡Ø§ÙƒØ±|hack)", re.I),
    re.compile(r"(ÙÙƒ|ÙØªØ­|Ø§Ø³ØªØ±Ø¬Ø§Ø¹).{0,10}(Ø­Ø³Ø§Ø¨|Ø­Ø¸Ø±|Ø¨Ø§Ø³ÙˆØ±Ø¯|ÙƒÙ„Ù…Ø© Ø³Ø±)", re.I),
    re.compile(r"(ØªØ¬Ø³Ø³|Ù…Ø±Ø§Ù‚Ø¨).{0,10}(ÙˆØ§ØªØ³|ÙˆØ§ØªØ³Ø§Ø¨|Ù‡Ø§ØªÙ|Ø¬ÙˆØ§Ù„|ØªÙ„ÙÙˆÙ†)", re.I),
    re.compile(r"(Ø³Ø­Ø¨|Ø³Ø±Ù‚Ø©).{0,10}(Ø¨ÙŠØ§Ù†Ø§Øª|Ù…Ø¹Ù„ÙˆÙ…Ø§Øª|Ø­Ø³Ø§Ø¨)", re.I),
]

# Unethical - sexual content, drugs, exploitation
UNETHICAL_PATTERNS = [
    re.compile(r"(Ø³ÙƒØ³|sex|porn|xxx|Ù†ÙŠÙƒ|Ø¨ÙˆØ±Ù†|Ù†ÙˆØ¯Ø²|nudes)", re.I),
    re.compile(r"(Ù‡ÙŠØ¬Ø§Ù†|Ù‡ÙŠØ¬Ø§Ù†Ù‡|Ø´Ù‡ÙˆÙ‡|Ø´Ù‡ÙˆØ©|Ù†ÙŠÙƒÙ†ÙŠ|Ø§Ù†ÙŠÙƒ)", re.I),
    re.compile(r"(ÙÙŠØ¯ÙŠÙˆ ÙƒÙˆÙ„|Ø³ÙƒØ³ Ø´Ø§Øª|video call).{0,10}(Ø¨Ù†Ø§Øª|Ø­Ø±ÙŠÙ…|Ù†Ø³Ø§Ø¡)", re.I),
    re.compile(r"(Ø­Ø´ÙŠØ´|Ø´Ø¨Ùˆ|ÙƒØ¨ØªØ§Ø¬ÙˆÙ†|ÙƒØ±ÙŠØ³ØªØ§Ù„|Ù…Ø®Ø¯Ø±|Ø­Ø¨ÙˆØ¨).{0,10}(Ù„Ù„Ø¨ÙŠØ¹|Ù…ØªÙˆÙØ±|Ù…ØªØ§Ø­|Ø¹Ù†Ø¯ÙŠ)", re.I),
    re.compile(r"(Ù‚Ø§ØµØ±|Ø§Ø·ÙØ§Ù„).{0,10}(Ø³ÙƒØ³|ØªØ­Ø±Ø´|Ø¬Ù†Ø³)", re.I),
]

# Normal indicators - things that suggest the text is actually normal conversation
NORMAL_INDICATORS = [
    re.compile(r"(Ù†ØµØ§Ø¨ÙŠÙ†|Ù†ØµØ§Ø¨|Ù…Ø­ØªØ§Ù„ÙŠÙ†|Ù…Ø­ØªØ§Ù„).{0,20}(Ø°ÙˆÙ„|Ù‡Ø¤Ù„Ø§Ø¡|Ù‡Ø¤Ù„Ø§)", re.I),
    re.compile(r"(Ø­Ø°Ø±|Ø§Ù†ØªØ¨Ù‡|ØªØ­Ø°ÙŠØ±|Ø®Ù„ÙˆØ§ Ø¨Ø§Ù„ÙƒÙ…)", re.I),
    re.compile(r"(Ù…Ø§ÙÙŠÙ‡ Ø§Ù„Ø§|Ø­Ù‚ÙŠÙ†|Ø­ÙˆÙ…ÙˆØ§ ÙƒØ¨Ø¯ÙŠ|ÙŠØ±Ø¬Ø§Ù„)", re.I),
    re.compile(r"(Ù‡Ù„|Ù…ØªÙ‰|ÙˆØ´|Ø§ÙŠØ´|ÙƒÙŠÙ).{0,30}(Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±|Ø§Ù…ØªØ­Ø§Ù†|Ø§Ø¹Ø°Ø§Ø±|Ø¹Ø°Ø±|ØªÙ‚Ø¯ÙŠÙ…)", re.I),
    re.compile(r"(Ø³Ø¨Ù‚|Ø§Ø­Ø¯|Ù‡Ù„).{0,20}(Ø§Ø®ØªØ¨Ø±|Ù‚Ø¯Ù…|Ø³Ø¬Ù„|Ø±ÙØ¹).{0,15}(Ø§Ø¹Ø°Ø§Ø±|Ø¹Ø°Ø±|ØªØ°ÙƒØ±)", re.I),
    re.compile(r"(Ø®Ø·ÙˆØ§Øª|Ø·Ø±ÙŠÙ‚Ø©|ÙƒÙŠÙ).{0,15}(Ø±ÙØ¹|ØªÙ‚Ø¯ÙŠÙ…).{0,10}(ØªØ°ÙƒØ±|Ø¹Ø°Ø±|Ø·Ù„Ø¨)", re.I),
    re.compile(r"(Ù…Ø´ÙƒÙ„Ø©|Ù…Ø´ÙƒÙ„Ù‡|Ù…ÙˆØ§Ø¬Ù‡|ØµØ¹ÙˆØ¨Ø©|ØµØ¹ÙˆØ¨Ù‡).{0,15}(Ù…Ø´Ø±ÙˆØ¹|ÙˆØ§Ø¬Ø¨|ØªØ³Ø¬ÙŠÙ„)", re.I),
    re.compile(r"(Ø¯Ø§Ù… Ø§Ù†|ØªØ±Ø§|Ø¨Ø§Ù„Ù†Ø³Ø¨Ø©|Ø¨Ø®ØµÙˆØµ).{0,20}(Ø§Ù„Ù…Ø´Ø§Ø±ÙŠ|Ø§Ù„Ø¨Ø­Ø«|Ø§Ù„Ø¯Ø±Ø§Ø³)", re.I),
    re.compile(r"(ÙˆØ§Ø®ØªØ¨Ø§Ø±Ø§Øª|Ø§Ø®ØªØ¨Ø§Ø±Ø§ØªÙ‡Ø§|Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª).{0,10}(Ø³Ù‡Ù„|ØµØ¹Ø¨|Ø­Ù„Ùˆ|Ø­Ù„ÙˆÙ‡|Ù…Ù…ØªØ§Ø²)", re.I),
    re.compile(r"(Ø§Ø¨Ø­Ø« Ø¹Ù†|Ø§Ø¨ÙŠ|Ø§Ø¨ØºÙ‰).{0,10}(ÙˆØ¸ÙŠÙ|Ø¹Ù…Ù„|Ø´ØºÙ„)", re.I),
    re.compile(r"(Ø§Ù†Ø§ Ø¨ØªØ®Ø±Ø¬|Ø¨Ø¹Ø¯ Ø§Ù„ØªØ®Ø±Ø¬|ÙˆØ´ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª)", re.I),
    re.compile(r"(Ø§Ø´ØªØ±ÙŠ|Ø§Ø¨ÙŠØ¹).{0,10}(Ø­Ø³Ø§Ø¨|Ø¬Ù‡Ø§Ø²|Ù„Ø§Ø¨ØªÙˆØ¨|Ø¬ÙˆØ§Ù„)", re.I),
]


def classify_text(text):
    """Return list of (category, confidence_reason) tuples that match the text."""
    matches = []
    
    # Check normal indicators first
    normal_match = False
    for p in NORMAL_INDICATORS:
        m = p.search(text)
        if m:
            normal_match = True
            matches.append(("Ø·Ø¨ÙŠØ¹ÙŠ", f"normal indicator: {m.group()}"))
    
    for p in MEDICAL_FRAUD_PATTERNS:
        m = p.search(text)
        if m:
            matches.append(("Ø§Ø­ØªÙŠØ§Ù„ Ø·Ø¨ÙŠ (Ø¹Ø±Ø¶)", f"medical: {m.group()}"))
    
    for p in ACADEMIC_OFFER_PATTERNS:
        m = p.search(text)
        if m:
            matches.append(("ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)", f"acad_offer: {m.group()}"))
    
    for p in ACADEMIC_REQUEST_PATTERNS:
        m = p.search(text)
        if m:
            matches.append(("ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø·Ù„Ø¨)", f"acad_request: {m.group()}"))
    
    for p in FINANCIAL_SCAM_PATTERNS:
        m = p.search(text)
        if m:
            matches.append(("Ø§Ø­ØªÙŠØ§Ù„ Ù…Ø§Ù„ÙŠ (Ø¹Ø±Ø¶)", f"financial: {m.group()}"))
    
    for p in SPAM_PATTERNS:
        m = p.search(text)
        if m:
            matches.append(("Ø³Ø¨Ø§Ù…", f"spam: {m.group()}"))
    
    for p in HACKING_PATTERNS:
        m = p.search(text)
        if m:
            matches.append(("ØªÙ‡ÙƒÙŠØ± (Ø¹Ø±Ø¶)", f"hacking: {m.group()}"))
    
    for p in UNETHICAL_PATTERNS:
        m = p.search(text)
        if m:
            matches.append(("ØºÙŠØ± Ø£Ø®Ù„Ø§Ù‚ÙŠ (Ø¹Ø±Ø¶)", f"unethical: {m.group()}"))
    
    return matches, normal_match


# Map English labels to Arabic broad categories for comparison
LABEL_FAMILY = {
    "Academic Cheating": "Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ",
    "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)": "Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ",
    "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø·Ù„Ø¨)": "Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ",
    "Medical Fraud": "Ø·Ø¨ÙŠ",
    "Ø§Ø­ØªÙŠØ§Ù„ Ø·Ø¨ÙŠ (Ø¹Ø±Ø¶)": "Ø·Ø¨ÙŠ",
    "Ø§Ø­ØªÙŠØ§Ù„ Ø·Ø¨ÙŠ (Ø·Ù„Ø¨)": "Ø·Ø¨ÙŠ",
    "Financial Scams": "Ù…Ø§Ù„ÙŠ",
    "Ø§Ø­ØªÙŠØ§Ù„ Ù…Ø§Ù„ÙŠ (Ø¹Ø±Ø¶)": "Ù…Ø§Ù„ÙŠ",
    "Ø§Ø­ØªÙŠØ§Ù„ Ù…Ø§Ù„ÙŠ (Ø·Ù„Ø¨)": "Ù…Ø§Ù„ÙŠ",
    "Spam": "Ø³Ø¨Ø§Ù…",
    "Ø³Ø¨Ø§Ù…": "Ø³Ø¨Ø§Ù…",
    "Hacking": "ØªÙ‡ÙƒÙŠØ±",
    "ØªÙ‡ÙƒÙŠØ± (Ø¹Ø±Ø¶)": "ØªÙ‡ÙƒÙŠØ±",
    "ØªÙ‡ÙƒÙŠØ± (Ø·Ù„Ø¨)": "ØªÙ‡ÙƒÙŠØ±",
    "Unethical": "ØºÙŠØ± Ø£Ø®Ù„Ø§Ù‚ÙŠ",
    "ØºÙŠØ± Ø£Ø®Ù„Ø§Ù‚ÙŠ (Ø¹Ø±Ø¶)": "ØºÙŠØ± Ø£Ø®Ù„Ø§Ù‚ÙŠ",
    "ØºÙŠØ± Ø£Ø®Ù„Ø§Ù‚ÙŠ (Ø·Ù„Ø¨)": "ØºÙŠØ± Ø£Ø®Ù„Ø§Ù‚ÙŠ",
    "Normal": "Ø·Ø¨ÙŠØ¹ÙŠ",
    "Ø·Ø¨ÙŠØ¹ÙŠ": "Ø·Ø¨ÙŠØ¹ÙŠ",
}

DETECTED_FAMILY = {
    "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)": "Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ",
    "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø·Ù„Ø¨)": "Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ",
    "Ø§Ø­ØªÙŠØ§Ù„ Ø·Ø¨ÙŠ (Ø¹Ø±Ø¶)": "Ø·Ø¨ÙŠ",
    "Ø§Ø­ØªÙŠØ§Ù„ Ù…Ø§Ù„ÙŠ (Ø¹Ø±Ø¶)": "Ù…Ø§Ù„ÙŠ",
    "Ø³Ø¨Ø§Ù…": "Ø³Ø¨Ø§Ù…",
    "ØªÙ‡ÙƒÙŠØ± (Ø¹Ø±Ø¶)": "ØªÙ‡ÙƒÙŠØ±",
    "ØºÙŠØ± Ø£Ø®Ù„Ø§Ù‚ÙŠ (Ø¹Ø±Ø¶)": "ØºÙŠØ± Ø£Ø®Ù„Ø§Ù‚ÙŠ",
    "Ø·Ø¨ÙŠØ¹ÙŠ": "Ø·Ø¨ÙŠØ¹ÙŠ",
}


def main():
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)

    print("=" * 70)
    print(f"ğŸ” Full Audit of ALL {len(data)} Samples")
    print("=" * 70)

    issues = []  # (index, issue_type, current_label, suggested_label, text, reason)
    
    for i, sample in enumerate(data):
        text = sample.get("text", "")
        label = sample.get("label", "")
        labels = sample.get("labels", [])
        label_family = LABEL_FAMILY.get(label, "unknown")
        
        detected, is_normal = classify_text(text)
        
        if not detected:
            # No patterns matched - if labeled as violation, might be wrong
            # but we can't be sure, so skip
            continue
        
        # Get detected categories (families)
        detected_families = set()
        detected_cats = set()
        for cat, reason in detected:
            detected_families.add(DETECTED_FAMILY.get(cat, cat))
            detected_cats.add(cat)
        
        # === ISSUE 1: Normal sample that matches violation patterns ===
        if label_family == "Ø·Ø¨ÙŠØ¹ÙŠ":
            violation_cats = [c for c in detected_cats if c != "Ø·Ø¨ÙŠØ¹ÙŠ"]
            if violation_cats and not is_normal:
                # Normal but matched violation pattern and no normal indicators
                reasons = [r for c, r in detected if c != "Ø·Ø¨ÙŠØ¹ÙŠ"]
                issues.append((i, "NORMAL_BUT_VIOLATION", label, violation_cats[0],
                               text, "; ".join(reasons[:2])))
        
        # === ISSUE 2: Violation sample that only matches Normal indicators ===
        elif label_family != "Ø·Ø¨ÙŠØ¹ÙŠ":
            if is_normal and len(detected_cats - {"Ø·Ø¨ÙŠØ¹ÙŠ"}) == 0:
                reasons = [r for c, r in detected if c == "Ø·Ø¨ÙŠØ¹ÙŠ"]
                issues.append((i, "VIOLATION_BUT_NORMAL", label, "Ø·Ø¨ÙŠØ¹ÙŠ",
                               text, "; ".join(reasons[:2])))
            
            # === ISSUE 3: Wrong violation category ===
            elif label_family not in detected_families and "Ø·Ø¨ÙŠØ¹ÙŠ" not in detected_families:
                # The detected category doesn't match the label family at all
                main_detected = [c for c in detected_cats if c != "Ø·Ø¨ÙŠØ¹ÙŠ"]
                if main_detected:
                    reasons = [r for c, r in detected if c != "Ø·Ø¨ÙŠØ¹ÙŠ"]
                    issues.append((i, "WRONG_CATEGORY", label, main_detected[0],
                                   text, "; ".join(reasons[:2])))
        
        # === ISSUE 4: Offer vs Request confusion (Academic only) ===
        if label in ["ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)", "Academic Cheating"]:
            if "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø·Ù„Ø¨)" in detected_cats and "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)" not in detected_cats:
                reasons = [r for c, r in detected if c == "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø·Ù„Ø¨)"]
                issues.append((i, "OFFER_IS_REQUEST", label, "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø·Ù„Ø¨)",
                               text, "; ".join(reasons[:2])))
        
        # === ISSUE 5: Spam labeled but really Academic Cheating ===
        if label_family == "Ø³Ø¨Ø§Ù…" and "Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ" in detected_families:
            reasons = [r for c, r in detected if DETECTED_FAMILY.get(c) == "Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"]
            issues.append((i, "SPAM_IS_ACADEMIC", label, "ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ (Ø¹Ø±Ø¶)",
                           text, "; ".join(reasons[:2])))
        
        # === ISSUE 6: Financial Scams labeled but really Spam ===
        if label_family == "Ù…Ø§Ù„ÙŠ" and "Ø³Ø¨Ø§Ù…" in detected_families and "Ù…Ø§Ù„ÙŠ" not in detected_families:
            reasons = [r for c, r in detected if DETECTED_FAMILY.get(c) == "Ø³Ø¨Ø§Ù…"]
            issues.append((i, "FINANCIAL_IS_SPAM", label, "Ø³Ø¨Ø§Ù…",
                           text, "; ".join(reasons[:2])))
        
        # === ISSUE 7: Academic labeled but primarily Medical ===
        if label_family == "Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ" and "Ø·Ø¨ÙŠ" in detected_families:
            medical_reasons = [r for c, r in detected if DETECTED_FAMILY.get(c) == "Ø·Ø¨ÙŠ"]
            academic_reasons = [r for c, r in detected if DETECTED_FAMILY.get(c) == "Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"]
            # If medical patterns come first in text or are more prominent
            if len(medical_reasons) >= len(academic_reasons):
                issues.append((i, "ACADEMIC_IS_MEDICAL", label, "Ø§Ø­ØªÙŠØ§Ù„ Ø·Ø¨ÙŠ (Ø¹Ø±Ø¶)",
                               text, "; ".join(medical_reasons[:2])))

    # Deduplicate issues (same index can appear multiple times)
    seen = set()
    unique_issues = []
    for issue in issues:
        idx = issue[0]
        if idx not in seen:
            seen.add(idx)
            unique_issues.append(issue)

    # Sort by issue type
    unique_issues.sort(key=lambda x: (x[1], x[0]))

    # Print results
    print(f"\nğŸ” Found {len(unique_issues)} potential issues:\n")
    
    by_type = defaultdict(list)
    for issue in unique_issues:
        by_type[issue[1]].append(issue)
    
    for issue_type, group in sorted(by_type.items()):
        type_names = {
            "NORMAL_BUT_VIOLATION": "ğŸŸ¡ Ø·Ø¨ÙŠØ¹ÙŠ Ù„ÙƒÙ† ÙŠØ­ØªÙˆÙŠ ÙƒÙ„Ù…Ø§Øª Ù…Ø®Ø§Ù„ÙØ©",
            "VIOLATION_BUT_NORMAL": "ğŸŸ¢ Ù…Ø®Ø§Ù„ÙØ© Ù„ÙƒÙ†Ù‡Ø§ ÙÙŠ Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø·Ø¨ÙŠØ¹ÙŠ",
            "WRONG_CATEGORY": "ğŸ”´ ÙØ¦Ø© Ø®Ø§Ø·Ø¦Ø©",
            "OFFER_IS_REQUEST": "ğŸ”µ Ø¹Ø±Ø¶ Ù„ÙƒÙ†Ù‡ ÙÙŠ Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø·Ù„Ø¨",
            "SPAM_IS_ACADEMIC": "ğŸŸ  Ø³Ø¨Ø§Ù… Ù„ÙƒÙ†Ù‡ ØºØ´ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ",
            "FINANCIAL_IS_SPAM": "ğŸŸ£ Ø§Ø­ØªÙŠØ§Ù„ Ù…Ø§Ù„ÙŠ Ù„ÙƒÙ†Ù‡ Ø³Ø¨Ø§Ù…",
            "ACADEMIC_IS_MEDICAL": "ğŸ©º Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ù„ÙƒÙ†Ù‡ Ø§Ø­ØªÙŠØ§Ù„ Ø·Ø¨ÙŠ",
        }
        print(f"\n{'='*60}")
        print(f"{type_names.get(issue_type, issue_type)} ({len(group)} Ø¹ÙŠÙ†Ø©)")
        print(f"{'='*60}")
        
        for idx, itype, curr_label, suggested, text, reason in group:
            print(f"\n  [{idx:4d}] {curr_label} â†’ {suggested}")
            print(f"         Ø§Ù„Ø³Ø¨Ø¨: {reason}")
            # Print full text, wrapped
            text_display = text.replace('\n', ' â†µ ')
            if len(text_display) > 120:
                print(f"         Ø§Ù„Ù†Øµ: {text_display[:120]}...")
            else:
                print(f"         Ø§Ù„Ù†Øµ: {text_display}")
    
    # Summary
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Ù…Ù„Ø®Øµ: {len(unique_issues)} Ø¹ÙŠÙ†Ø© ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©")
    for itype, group in sorted(by_type.items()):
        type_short = {
            "NORMAL_BUT_VIOLATION": "Ø·Ø¨ÙŠØ¹ÙŠâ†’Ù…Ø®Ø§Ù„ÙØ©",
            "VIOLATION_BUT_NORMAL": "Ù…Ø®Ø§Ù„ÙØ©â†’Ø·Ø¨ÙŠØ¹ÙŠ",
            "WRONG_CATEGORY": "ÙØ¦Ø© Ø®Ø§Ø·Ø¦Ø©",
            "OFFER_IS_REQUEST": "Ø¹Ø±Ø¶â†’Ø·Ù„Ø¨",
            "SPAM_IS_ACADEMIC": "Ø³Ø¨Ø§Ù…â†’Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ",
            "FINANCIAL_IS_SPAM": "Ù…Ø§Ù„ÙŠâ†’Ø³Ø¨Ø§Ù…",
            "ACADEMIC_IS_MEDICAL": "Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠâ†’Ø·Ø¨ÙŠ",
        }
        print(f"  {type_short.get(itype, itype)}: {len(group)}")

    # Output as JSON for further processing
    output = []
    for idx, itype, curr_label, suggested, text, reason in unique_issues:
        output.append({
            "index": idx,
            "issue_type": itype,
            "current_label": curr_label,
            "suggested_label": suggested,
            "reason": reason,
            "text_preview": text[:80]
        })
    
    with open("scripts/audit_report_v2.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„: scripts/audit_report_v2.json")


if __name__ == "__main__":
    main()
